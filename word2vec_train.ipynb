{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica4\\word-embeddings\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Step 1: Load the dataset and fetch only the first 100 MB of data\n",
    "data = load_dataset(\"projecte-aina/catalan_general_crawling\", split='train', trust_remote_code=True)\n",
    "\n",
    "# Function to tokenize a chunk of text\n",
    "def tokenize_chunk(chunk):\n",
    "    return [word_tokenize(text.lower()) for text in chunk]\n",
    "\n",
    "# Function to process data in chunks\n",
    "def process_chunks(data_subset, chunk_size):\n",
    "    tokenized_data = []\n",
    "    for i in range(0, len(data_subset), chunk_size):\n",
    "        chunk = data_subset[i:i + chunk_size]\n",
    "        tokenized_data.extend(tokenize_chunk(chunk))\n",
    "    return tokenized_data\n",
    "\n",
    "# Define chunk size (adjust as needed)\n",
    "chunk_size = 1000\n",
    "\n",
    "# Fetching the first 100 MB of data in chunks\n",
    "data_subset = []\n",
    "total_size = 0\n",
    "for example in data:\n",
    "    text = example['text']\n",
    "    text_size_mb = len(text.encode('utf-8')) / (1024 * 1024) # Convert bytes to MB\n",
    "    if total_size + text_size_mb <= 100:\n",
    "        data_subset.append(text.lower())\n",
    "        total_size += text_size_mb\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Tokenize the data subset in chunks\n",
    "tokenized_data_nosent = process_chunks(data_subset, chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the Word2Vec model\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=tokenized_data_nosent,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=cpu_count(),\n",
    "    epochs=25\n",
    ")\n",
    "\n",
    "# Example: Obtain a word vector\n",
    "word_vector = model.wv['example']  # Replace 'example' with a word from your vocabulary\n",
    "print(word_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m joined_subset\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdata_subset\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_subset' is not defined"
     ]
    }
   ],
   "source": [
    "joined_subset= \" \".join(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Tokenize the text data\n",
    "tokenized_data = [word_tokenize(sent)  for sent in sent_tokenize(joined_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = word2vec.Word2Vec(tokenized_data_nosent, vector_size=100, window=5, min_count=3, workers=16, epochs=25)\n",
    "# Obtenir un word-vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/word2vec_1000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"./models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05194367  0.09216756  0.14081635 -0.00480924 -0.01386066 -0.31191224\n",
      "  0.08968573  0.18847178  0.06922817 -0.15064259  0.10461249 -0.08052815\n",
      " -0.08613951 -0.08889796 -0.00346578 -0.12006641  0.25400966 -0.03482381\n",
      " -0.04711282 -0.367745    0.02739826  0.00981252  0.20694314 -0.03951041\n",
      "  0.08894076 -0.00355345 -0.20581357  0.00096781 -0.02978049  0.0190071\n",
      "  0.19779688 -0.12933686 -0.09991755 -0.06531731 -0.03510002  0.02269764\n",
      " -0.0252097   0.08014361 -0.09204853 -0.01647645  0.03562705  0.06043269\n",
      "  0.06700999  0.08659964 -0.01389605 -0.08491652 -0.0138585  -0.03335764\n",
      " -0.10989612 -0.04486772 -0.2032342  -0.25091216  0.10321251 -0.11810295\n",
      " -0.10623726  0.14212777 -0.06104031 -0.24451292 -0.17541908  0.09244593\n",
      " -0.07245418 -0.4064446  -0.00462046  0.10750557 -0.25456852  0.1260386\n",
      " -0.14866126 -0.01792279 -0.29813674 -0.09827907  0.13119906  0.18624447\n",
      "  0.19752896  0.10422666 -0.08360326  0.07444219  0.11293318 -0.07011721\n",
      " -0.09176433 -0.05731177  0.04093094 -0.11248876 -0.04604669  0.11421655\n",
      "  0.0344146  -0.11679483  0.11300928  0.05184853 -0.10160217  0.07552502\n",
      "  0.12068627  0.03224142  0.19641076 -0.03854448  0.25377676  0.10367408\n",
      " -0.15360962 -0.19758652  0.28879878 -0.10564435]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv[\"islam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('antoni', 0.8435072302818298),\n",
       " ('valentí', 0.8416920900344849),\n",
       " ('francesc', 0.8236792683601379),\n",
       " ('ignasi', 0.7941128015518188),\n",
       " ('manel', 0.791876494884491),\n",
       " ('jaume', 0.7878979444503784),\n",
       " ('pere', 0.7868216633796692),\n",
       " ('jordi', 0.7865138649940491),\n",
       " ('bernat', 0.7773346900939941),\n",
       " ('vicenç', 0.7647559642791748)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"miquel\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_nosent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/tokenized1gb.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenized_data_nosent, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pelé', 0.7629638314247131),\n",
       " ('quini', 0.7298490405082703),\n",
       " ('ronaldinho', 0.7237531542778015),\n",
       " ('leo', 0.7187958359718323),\n",
       " ('figo', 0.7079043984413147),\n",
       " ('riquelme', 0.7039819955825806),\n",
       " ('messi', 0.7022386789321899),\n",
       " ('pirlo', 0.6906698942184448),\n",
       " (\"l'argentí\", 0.6814577579498291),\n",
       " ('belletti', 0.6788095831871033)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"maradona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"./models/word2vec_1000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PearsonRResult(statistic=0.39531623392458726, pvalue=1.3150681762230595e-09), SignificanceResult(statistic=0.4189640612241074, pvalue=1.016457600596369e-10), 0.45454545454545453)\n"
     ]
    }
   ],
   "source": [
    "analogies_result = model.wv.evaluate_word_pairs('wordsim353.en.ca.txt')\n",
    "print(analogies_result) #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
